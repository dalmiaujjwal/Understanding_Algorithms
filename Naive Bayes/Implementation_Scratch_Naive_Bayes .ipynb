{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier - Implementation from scratch \n",
    "The notbook contains the code to implement the Naive Bayes Classifier from Scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the dataset\n",
    "- This is a kaggle dataset which has news text articles along with the category they belong to like business, sports etc.\n",
    "- The objective is to make a classifier which can predict the category of news article given it's text\n",
    "- The sample records of the dataset are shown for your reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1833</td>\n",
       "      <td>worldcom ex-boss launches defence lawyers defe...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154</td>\n",
       "      <td>german business confidence slides german busin...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ArticleId                                               Text  Category\n",
       "0       1833  worldcom ex-boss launches defence lawyers defe...  business\n",
       "1        154  german business confidence slides german busin...  business"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_dataset = pd.read_csv(\"C:\\\\Ujjwal\\\\Analytics\\\\Datasets\\\\News Classification\\\\News_train.csv\")\n",
    "inp_dataset.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the articles to remove the unwanted characters\n",
    "- In this step we are cleaning the dataset and dividing it in to test and train set\n",
    "- The data cleaning is done in the function defined as **text_clean**. Given the focus is on identifying popular words which come in specific category of articles, the function removes all the numbers, puncutations and special characters from the text. Moreover, any extra spaces are replaced with single space.\n",
    "- Finally, we are splitting the data in to train and test set using *sklearn's function* **test_train_split**\n",
    "- Notice the number of records in original, training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_clean(text_series):\n",
    "    clean_1 = text_series.str.replace(r\"[^a-zA-Z\\s]\",\"\")\n",
    "    clean_2 = clean_1.str.replace(r\"\\s+\", \" \")\n",
    "    return clean_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_dataset[\"Text_Clean\"] = text_clean(inp_dataset[\"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = inp_dataset['Category']\n",
    "train_x, test_x, train_y, test_y  = train_test_split(inp_dataset,Y,random_state = 8)\n",
    "train_x.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in original dataset - 1490\n",
      "Number of records in training dataset - 1117\n",
      "Number of records in testing dataset - 373\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of records in original dataset - {inp_dataset.shape[0]}\\nNumber of records in training dataset - {train_x.shape[0]}\\nNumber of records in testing dataset - {test_x.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Bag of Words\n",
    "- In this step we are creating a dataframe which has the count frequency of each word in each document\n",
    "- This is done using *sklearn's* **CountVectorizer** function. We have **passed the argument stop_words as \"english\" so that the function can remove the english stop words on it's own**.\n",
    "- Observe the sample records from the dataframe. The number 1 below abacus in record 4 indicates that the **word Abacus came once in this document**. \n",
    "\n",
    "**Note:**\n",
    "\n",
    "- Since the number of unique words across all the documents are very high, all the words are not visible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaas</th>\n",
       "      <th>aac</th>\n",
       "      <th>aadc</th>\n",
       "      <th>aaliyah</th>\n",
       "      <th>aamir</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aashare</th>\n",
       "      <th>abacus</th>\n",
       "      <th>...</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zooms</th>\n",
       "      <th>zooropa</th>\n",
       "      <th>zuluaga</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zutons</th>\n",
       "      <th>zvonareva</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 22164 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aa  aaa  aaas  aac  aadc  aaliyah  aamir  aaron  aashare  abacus  ...  \\\n",
       "4   0    0     0    0     0        0      0      0        0       1  ...   \n",
       "5   0    0     0    0     0        0      0      0        0       0  ...   \n",
       "6   0    0     0    0     0        0      0      0        0       0  ...   \n",
       "7   0    0     0    0     0        0      0      0        0       0  ...   \n",
       "\n",
       "   zombies  zone  zones  zoom  zooms  zooropa  zuluaga  zurich  zutons  \\\n",
       "4        0     0      0     0      0        0        0       0       0   \n",
       "5        0     0      0     0      0        0        0       0       0   \n",
       "6        0     0      0     0      0        0        0       0       0   \n",
       "7        0     0      0     0      0        0        0       0       0   \n",
       "\n",
       "   zvonareva  \n",
       "4          0  \n",
       "5          0  \n",
       "6          0  \n",
       "7          0  \n",
       "\n",
       "[4 rows x 22164 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cnt_Vec = CountVectorizer(stop_words=\"english\")\n",
    "BOW = Cnt_Vec.fit_transform(train_x[\"Text_Clean\"]).toarray()\n",
    "BOW_Df = pd.DataFrame(BOW, columns=Cnt_Vec.get_feature_names())\n",
    "BOW_Df[4:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Dataframes & Creating training and test set\n",
    "- To get the article category values along with the word frequency, we are merging the 2 datasets\n",
    "- This is done because we will be implementing the Naive Bayes approach from scratch and not using sklearn's inbuilt functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "      <th>Text_Clean</th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaas</th>\n",
       "      <th>aac</th>\n",
       "      <th>aadc</th>\n",
       "      <th>aaliyah</th>\n",
       "      <th>...</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zooms</th>\n",
       "      <th>zooropa</th>\n",
       "      <th>zuluaga</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zutons</th>\n",
       "      <th>zvonareva</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>912</td>\n",
       "      <td>warning over tsunami aid website net users are...</td>\n",
       "      <td>tech</td>\n",
       "      <td>warning over tsunami aid website net users are...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>901</td>\n",
       "      <td>kenteris denies faking road crash greek sprint...</td>\n",
       "      <td>sport</td>\n",
       "      <td>kenteris denies faking road crash greek sprint...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 22168 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ArticleId                                               Text Category  \\\n",
       "0        912  warning over tsunami aid website net users are...     tech   \n",
       "1        901  kenteris denies faking road crash greek sprint...    sport   \n",
       "\n",
       "                                          Text_Clean  aa  aaa  aaas  aac  \\\n",
       "0  warning over tsunami aid website net users are...   0    0     0    0   \n",
       "1  kenteris denies faking road crash greek sprint...   0    0     0    0   \n",
       "\n",
       "   aadc  aaliyah  ...  zombies  zone  zones  zoom  zooms  zooropa  zuluaga  \\\n",
       "0     0        0  ...        0     0      0     0      0        0        0   \n",
       "1     0        0  ...        0     0      0     0      0        0        0   \n",
       "\n",
       "   zurich  zutons  zvonareva  \n",
       "0       0       0          0  \n",
       "1       0       0          0  \n",
       "\n",
       "[2 rows x 22168 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_dataset_final = pd.merge(train_x, BOW_Df, left_index=True, right_index=True, how = \"left\")\n",
    "inp_dataset_final.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolidating Bag of Words on the basis of News Categories\n",
    "- In this section we are grouping all the word frequencies on the basis of different news categories they fall in\n",
    "- An additional column is added to the dataframe which contains the count of all the words across the documents in specific category\n",
    "- The ArticleId column is having the count of articles in each category\n",
    "- Refer to the sample records as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaas</th>\n",
       "      <th>aac</th>\n",
       "      <th>aadc</th>\n",
       "      <th>aaliyah</th>\n",
       "      <th>aamir</th>\n",
       "      <th>aaron</th>\n",
       "      <th>...</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zooms</th>\n",
       "      <th>zooropa</th>\n",
       "      <th>zuluaga</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zutons</th>\n",
       "      <th>zvonareva</th>\n",
       "      <th>sum_all_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>245</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>211</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>politics</td>\n",
       "      <td>208</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>48353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>261</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tech</td>\n",
       "      <td>192</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>48323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Category  ArticleId  aa  aaa  aaas  aac  aadc  aaliyah  aamir  aaron  \\\n",
       "0       business        245   0    0     0    0     0        0      0      0   \n",
       "1  entertainment        211   0    0     0    0     0        4      1      0   \n",
       "2       politics        208   1    0     0    0     0        0      0      0   \n",
       "3          sport        261   0    2     1    3     0        0      0      2   \n",
       "4           tech        192   0    0     0    0     3        0      0      0   \n",
       "\n",
       "   ...  zone  zones  zoom  zooms  zooropa  zuluaga  zurich  zutons  zvonareva  \\\n",
       "0  ...     1      3     0      0        0        0       1       0          0   \n",
       "1  ...     0      0     0      0        1        0       0       1          0   \n",
       "2  ...     0      0     0      0        0        0       0       0          0   \n",
       "3  ...     4      0     0      0        0        1       4       0          1   \n",
       "4  ...     1      1     3      1        0        0       0       0          0   \n",
       "\n",
       "   sum_all_words  \n",
       "0          42126  \n",
       "1          35964  \n",
       "2          48353  \n",
       "3          42686  \n",
       "4          48323  \n",
       "\n",
       "[5 rows x 22167 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cons_dataset = inp_dataset_final.groupby(\"Category\").agg({col:\"count\" if col == \"ArticleId\" else \"sum\" for col in inp_dataset_final.columns})\n",
    "cols = [col for col in cons_dataset.columns if col not in [\"Text\", \"Category\", \"Text_Clean\"]]\n",
    "cons_dataset_updated = cons_dataset[cols].copy()\n",
    "cons_dataset_updated.reset_index(inplace=True)\n",
    "cons_dataset_updated[\"sum_all_words\"] = cons_dataset_updated.iloc[:,2:].apply(lambda x: np.sum(x), axis=1)\n",
    "cons_dataset_updated.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Probability Table\n",
    "- Using the consolidated table we created in the previous section, we will now calculate the probabilities of each word occuring in each artcile category\n",
    "- Laplace smoothing is also built in to ensure that words with 0 frequency are taken care of. This table will be used to calculate the probability of all the test documents\n",
    "- See the sample dataframe output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "prob_table = pd.DataFrame()\n",
    "prob_table[\"Category\"] = cons_dataset_updated[\"Category\"]\n",
    "prob_table[\"p_C\"] = cons_dataset_updated[\"ArticleId\"]/cons_dataset_updated.shape[0]\n",
    "cols = [col for col in cons_dataset_updated.columns if col not in [\"Category\", \"ArticleId\", \"sum\"]]\n",
    "no_of_cols = len(cols)\n",
    "for col in cols:\n",
    "    prob_table[col] = (cons_dataset_updated[col]+alpha)/(cons_dataset_updated[\"sum_all_words\"] + (alpha*no_of_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>p_C</th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaas</th>\n",
       "      <th>aac</th>\n",
       "      <th>aadc</th>\n",
       "      <th>aaliyah</th>\n",
       "      <th>aamir</th>\n",
       "      <th>aaron</th>\n",
       "      <th>...</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zooms</th>\n",
       "      <th>zooropa</th>\n",
       "      <th>zuluaga</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zutons</th>\n",
       "      <th>zvonareva</th>\n",
       "      <th>sum_all_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.655265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>42.2</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.618721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 22166 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Category   p_C        aa       aaa      aaas       aac      aadc  \\\n",
       "0       business  49.0  0.000016  0.000016  0.000016  0.000016  0.000016   \n",
       "1  entertainment  42.2  0.000017  0.000017  0.000017  0.000017  0.000017   \n",
       "\n",
       "    aaliyah     aamir     aaron  ...      zone     zones      zoom     zooms  \\\n",
       "0  0.000016  0.000016  0.000016  ...  0.000031  0.000062  0.000016  0.000016   \n",
       "1  0.000086  0.000034  0.000017  ...  0.000017  0.000017  0.000017  0.000017   \n",
       "\n",
       "    zooropa   zuluaga    zurich    zutons  zvonareva  sum_all_words  \n",
       "0  0.000016  0.000016  0.000031  0.000016   0.000016       0.655265  \n",
       "1  0.000034  0.000017  0.000017  0.000034   0.000017       0.618721  \n",
       "\n",
       "[2 rows x 22166 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_table.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a word tokenizer\n",
    "- Here we are defining a word tokenizer function to split the test articles in to individual words\n",
    "- We could have used our count vectorizer variable to transform the text but just to demonstrate the process of tokenizing, this funciton is defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wt(text):\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\",\" \", text)\n",
    "    list_of_words = text.split(\" \")\n",
    "    return list_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the classification\n",
    "- Using the probability table creating in the previous section, the test dataset articles will be classified\n",
    "- As can be seen fromt he results, close to 95% of the documents got correctly classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of text documents evaluated - 173\n",
      "Number of documents correctly classified - 164\n",
      "Number of documents incorrectly classified - 9\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "incorrect = 0\n",
    "for i in range(0,test_x.shape[0]-200):\n",
    "    text = test_x.loc[i,\"Text_Clean\"]\n",
    "    prob = pd.DataFrame()\n",
    "    prob[\"Category\"] = prob_table[\"Category\"]\n",
    "    prob[\"prob\"] = prob_table['p_C']\n",
    "    for val in wt(text):\n",
    "        if val in prob_table.columns:\n",
    "            prob[\"prob\"] = prob[\"prob\"] * prob_table[val] * 1000\n",
    "        else:\n",
    "            prob[\"prob\"] = prob[\"prob\"] * 1\n",
    "    prob[\"probability\"] = prob[\"prob\"]/prob[\"prob\"].sum()\n",
    "    prob.sort_values(\"probability\",ascending = False, inplace=True)\n",
    "    if test_x.loc[i,'Category'] == prob.iloc[0,0]:\n",
    "        correct += 1\n",
    "    else:\n",
    "        incorrect +=1\n",
    "print(f'Total number of text documents evaluated - {test_x.shape[0]-200}\\nNumber of documents correctly classified - {correct}\\nNumber of documents incorrectly classified - {incorrect}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
